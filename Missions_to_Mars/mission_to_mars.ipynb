{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'splinter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-22d09fefa68e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msplinter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBrowser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'splinter'"
     ]
    }
   ],
   "source": [
    "# Import Dependencies\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from splinter import Browser\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use requests and BeautifulSoup to scrape Nasa News for latest news\n",
    "url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'\n",
    "response = requests.get(url)\n",
    "soup = bs(response.text, 'lxml')\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = soup.find('div', class_='features')\n",
    "# for result in results:\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_title = results.find('div', class_='content_title').text\n",
    "newsp = results.find('div', class_='rollover_description').text\n",
    "print(news_title)\n",
    "print(newsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find image url for current featured mars image\n",
    "executable_path = {'executable_path': '/Users/venessayeh/Downloads/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "nasa_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "browser.visit(nasa_url)\n",
    "\n",
    "nasa_html = browser.html\n",
    "nasa_soup = bs(nasa_html, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featured = nasa_soup.find('div', class_='default floating_text_area ms-layer')\n",
    "featured_image = featured.find('footer')\n",
    "featured_image_url = 'https://www.jpl.nasa.gov'+ featured_image.find('a')['data-fancybox-href']\n",
    "print(str(featured_image_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape Mars Weather Twitter account for latest weather report on Mars\n",
    "twitter_url = 'https://twitter.com/marswxreport?lang=en'\n",
    "twitter_response = requests.get(twitter_url)\n",
    "twitter_soup = bs(twitter_response.text, 'lxml')\n",
    "twitter_result = twitter_soup.find('div', class_='js-tweet-text-container')\n",
    "# for tweet in twitter_result:\n",
    "#     print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_weather = twitter_result.find('p', class_='js-tweet-text').text\n",
    "mars_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape space-facts.com for mars fact using Pandas read_html function\n",
    "mars_facts_url = 'https://space-facts.com/mars/'\n",
    "tables = pd.read_html(mars_facts_url)\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pandas dataframe\n",
    "df = tables[0]\n",
    "df.columns = ['Description', 'Value']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index \n",
    "df.set_index('Description', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export pandas df to html script\n",
    "mars_facts = df.to_html()\n",
    "mars_facts.replace(\"\\n\", \"\")\n",
    "df.to_html('mars_facts.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape astrogeology.usgs.gov for hemisphere image urls and titles\n",
    "hemisphere_url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "browser.visit(hemisphere_url)\n",
    "\n",
    "hemisphere_html = browser.html\n",
    "hemisphere_soup = bs(hemisphere_html, 'lxml')\n",
    "base_url =\"https://astrogeology.usgs.gov\"\n",
    "\n",
    "image_list = hemisphere_soup.find_all('div', class_='item')\n",
    "\n",
    "# Create list to store dictionaries of data\n",
    "hemisphere_image_urls = []\n",
    "\n",
    "# Loop through each hemisphere and click on link to find large resolution image url\n",
    "for image in image_list:\n",
    "    hemisphere_dict = {}\n",
    "    \n",
    "    href = image.find('a', class_='itemLink product-item')\n",
    "    link = base_url + href['href']\n",
    "    browser.visit(link)\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    hemisphere_html2 = browser.html\n",
    "    hemisphere_soup2 = bs(hemisphere_html2, 'lxml')\n",
    "    \n",
    "    img_title = hemisphere_soup2.find('div', class_='content').find('h2', class_='title').text\n",
    "    hemisphere_dict['title'] = img_title\n",
    "    \n",
    "    img_url = hemisphere_soup2.find('div', class_='downloads').find('a')['href']\n",
    "    hemisphere_dict['url_img'] = img_url\n",
    "    \n",
    "    # Append dictionary to list\n",
    "    hemisphere_image_urls.append(hemisphere_dict)\n",
    "      \n",
    "hemisphere_image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 36 (PythonData)",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
